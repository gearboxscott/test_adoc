ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

include::vars/render-vars.adoc[]
include::vars/document-vars.adoc[]
include::vars/redhat-vars.adoc[]
include::vars/customer-vars.adoc[]
include::locale/attributes.adoc[]

// Adding logo to the coverpage.
// Change the pdfwidth to improve embedding
// ifeval::["{customerlogo}" != "empty"]
// :title-logo-image: image:{customerlogo}[pdfwidth=50%,align=center]
// endif::[]

= {subject}

{customer}

{description}

:toc:

== The Story

A task was presented to create a hardened base image for various cloud virtualization platforms using Ansible, Ansible Automation Platform and Packer.

Packer is dispositioned to use OS isos to build servers with either kickstart configuration files for Linux-based servers and auto unattended files for Windows-based servers. Kickstart and Auto Unattended files can do a lot things during the initialization like add and remove packages, create users, partition disk, etc. It does have its limitations. After those files are done, they cannot be used again without rebuilding the entire images.

Using Ansible can help here to finish out the build and allow the configuration files to remain in their simpliest state. Ansible can harden the image and coded to with roles that can use on the image and after it been deployed to keep the hardening in place. It can be variablized to allow change that need to happen over time to made easily. Having reusability is one of strengths.

This section will show how Packer will build the initial image, then it will reach out to Ansible Automation Platform to finish up the initial image to install and remove packages, setup and disable users, permissions and fortifications so the image at rest is already hardened before it's deployed in the cloud environment.

The other part of this story is the Packer and Automation workflow is able to produce hardened base images for all types of cloud platforms like VMware, Azure, Amazon and Oracle Cloud.

== Process Flow

##TODO##  Insert process flow diagram here.

== Requirements

The following is a list of requirements that makes this process work:

* A custom execution environment will be required that has Packer installed. The details are in the `Ansible Custom Execution Environment` section below.

== Components

== Ansible Custom Execution Environment

The following are files that were used to create the custom execution environment:

The `ansible.cfg` will allow the ansible builder to have access to the Private Automation to pull down collections, etc.

.custom-execution-environment/ansible.cfg
[source,yaml,source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/ansible.cfg[]
----

This file determines which collection will be preloaded on to the custom execution environment.

.custom-execution-environment/requirements.yml
[source,yaml,source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/requirements.yml[]
----

This file will make sure required python modules for roles and collections of roles are installed.

.custom-execution-environment/requirements.txt
[source,ini,source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/requirements.txt[]
----

The system entry in the definition points to a bindep requirements file, which will install system-level dependencies that are outside of what the collections already include as their dependencies. It may be listed as a relative path from the directory of the automation execution environment definition’s folder, or an absolute path.

.custom-execution-environment/bindep.txt
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/bindep.txt[]
----

The `execution-environment.yml` is the main file that tells the Ansible Builder how to build the custom execution environment:

.custom-execution-environment/execution-environment.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/execution-environment.yml[]
----

After these file were created and it's recommended that these file are storaged in a repository under version control. These files will be updated over time to update the version of all the components that on this execution environment. The next step is to build the new custom execution environment. The following are the step used to build the custom execution environment:

Steps:

. Login into the Private Automation Hub's registry using `podman`:
+
.Log into Private Automation Hub registry.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
$ podman login -u=username automation-hub-url
----

. Pull down a current Execution Environment like ee-supported-rhel8:latest:
+
.Use podman to pull ee-supported-rhel8:latest.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
podman pull https://{automation_hub}/ee-supported-rhel8:latest
----

. Login into the Private Automation Hub's registry using `podman`:
+
.Build the new execution environment with ansible-builder.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
$ ansible-builder build --file execution-environment.yml --verbosity 3 --container-runtime=podman --tag custom-supported-ee:1.0 --tag custom-supported-ee:latest
----

. Obtain the IMAGE ID for the image that the ansible-builder just generated:
+
.Obtain the image id for the new custom execution environment.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
$ podman push images
----

. Pick the image id for `localhost/custom-supported-ee`. Push the new execution environment to the Hub using the image id:
+
.Log into Private Automation Hub registry.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
$ podman push <image_id> automationhub/custom-supported-ee
----

. Log into the Controller to add the custom execution environment.

. Click Execution Environments.

. Click **Add** to add new environment.

. Fill in **Name** with `Custom Supported EE`.

. Fill in **Image** with `{automation_hub}/custom-suppor-ee:latest`.

. Select `Always pull container before running` for **Pull**.

. Fill in **Description** with `Use for packer image builds`.

. Leave **Organaization** blank.

. Select `Automation Hub Container Registry` for **Registry credential**.

The Custom Execution Environment is ready to use for building images.

== Inventory Variable Setup

The best way to allow this process to be changed over time is to use variables in all the roles, packer templates, ansible templates, etc. This requires the setup of inventory that will use in the Controller that has variables. The inventory is a project repository that is called into inventory as a source.

The directory tree for inventory look like the following:

[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
.
├── group_vars
│   ├── all
│   │   └── 00-shared.yml
│   ├── linux
│   │   ├── 00-registration.yml
│   │   ├── 00-rhel7-cis.yml
│   │   ├── 00-rhel8-cis.yml
│   │   └── 00-shared-linux.yml
│   ├── win2019
│   │   └── 00-windows-2019-cis.yml
│   └── windows
│       ├── 00-shared-windows.yml
│       └── all.yml
├── host_vars
└── inventory
----

The inventory is setup so there are two groups that used to provision images. Those groups are `linux` and `windows`. Looking at the inventory directory list above, windows has two groups because windows server by the nature have different building requirements. `window2019` is one example that has variable specific to that image. The general `windows` variables file hold variables that are common amoungst all windows images.

Linux could have been done the same way to split them out for images like RHEL, OEL, Ubuntu, etc. For most cases, Linux variations have very little if any uncommon settings other then repository differences.

The other files in those groups of OSes are for different packages that need to be preloaded on the image to make it available for deployment later in Cloud infrastructure.

If the file is only under `linux` then it's only used during linux builds and that's same for windows.

Let look at some of key files in the inventory variable source.

=== File: `00-shared.yml`

This file contains variables for Jinja2 templates that will create Packer Template HCL files, Kickstart Configuration Files, etc.

.00-shared.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-shared.yml[]
----
<1> Packer will show information in color but when executing it from AAP, those color codes will not render correctly and really makes a mess in the Job Log.

=== File: `00-shared-linux.yml``

.00-shared-linux.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-shared-linux.yml[]
----
<1> After the partitions are hardened, one parameter call `noexec` will keep Ansible from running on the new image, so a new path for executing Ansible will need to be provided.

=== File: `00-shared-windows.yml``

.00-shared-windows.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-shared-windows.yml[]
----
<1> Provide Ansible with a method of connecting to Windows to execute Ansible playbooks and roles.
<2> After the OS is built, Windows is patch using the categories listed.
<3> Determine whether to reboot Windows after patching or not.
<4> How long to wait for the reboot to complete and return access to Windows for Ansible to continue.
<5> Where to log the patching activities.

=== File: `inventory`

This file is used by Inventory Sources, that has been set to `Sourced from a Project`. The inventory variables are upload to the controller as a Git repository via a Project.  Once the project is synced then a Inventory Source can be create, using `inventory` as `Inventory File` setting.

.inventory.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/inventory[]
----

=== File: `00-rhel7-cis.yml`

This inventory file is used by the role `RHEL7-CIS` which is from `https://github.com/ansible-lockdown/RHEL7-CIS`. There are variable in this file that need to configured so Ansible and Packer is able to build the RHEL 7 image. Some of the rule were disabled to keep the Image from stopping Ansible from completing it's build. When the image is actually use, the role will need to executed again, with those rules enabled as the last role after all the connection to LDAP, application are installed, etc.

.00-rhel7-cis.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-rhel7-cis.yml[]
----

=== File: `00-rhel8-cis.yml`

his inventory file is used by the role `RHEL8-CIS` which is from `https://github.com/ansible-lockdown/RHEL8-CIS`. There are variable in this file that need to configured so Ansible and Packer is able to build the RHEL 8 image. Some of the rule were disabled to keep the Image from stopping Ansible from completing it's build. When the image is actually use, the role will need to executed again, with those rules enabled as the last role after all the connection to LDAP, application are installed, etc.

.00-rhel8-cis.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-rhel8-cis.yml[]
----

=== File: `00-windows-2019-cis.yml``

.00-windows-2019-cis.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-windows-2019-cis.yml[]
----

== Ansible Project and Roles

The process of building the image consists of two parts a project and it calls a role using the variables above and the custom execution environment that has packer pre-installed. The project or playbook has all the Job Templates that will be called by Ansible Controller. The role is doing all image building, the project is there call the image build role and how the post-image setup playbooks that are used by the Job Templates.

Each Job Template in the project call in various Workflow Job Templates in a pre-determined order to finish the image setup. After the image is setup then it's converted back to a VMware template. The VMware template can be converted later to be used on other clouds, but initially it's all build on VMware.

The `site.yml`` in the project called `Packer-OS-Base-Image` contains the following:

.'Packer-OS-Base-Image/site.yml`.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/site.yml[]
----
<1> `hosts;` is set to `localhost` because this run directly on the execution node.
<2> `connection:` is set to 'local' because this run directly on the execution node.
<3> All these variable are passed into the playbook via Survey questions ansible in the Job Template on the Ansible Controller.

Once the Job Template is executed, it will use the variables from the Job Template Survey question and call the role `Role-Packer-OS-Base-Image`. This role uses Packer to build the image, then called Ansible Controller to call Workflow Job Templates to setup the image be hardened, etc.

.'Role-Packer-OS-Base-Image/main.yml`.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/main.yml[]
----

== Packer Setup

Put stuff here to the packer setup



== Ansible Controller

== Variables

== Play by Play

== How to make changes and updates

== Final Thoughts

== Appendix

===

This Job Template can be import into Ansible Controller using `awx import` which is part of the `awxkit` python module.

.Base Image OS Templates Configuration
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/job_templates/Base-Image-OS-Job-Template.yml[]
----