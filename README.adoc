
include::vars/render-vars.adoc[]
include::vars/document-vars.adoc[]
include::vars/redhat-vars.adoc[]
include::vars/customer-vars.adoc[]
include::locale/attributes.adoc[]

Adding logo to the coverpage.
Change the pdfwidth to improve embedding
ifeval::["{customerlogo}" != "empty"]
:title-logo-image: image:{customerlogo}[pdfwidth=50%,align=center]
endif::[]

= {subject}

{customer}

{description}

toc::[]

== The Story

A task was presented to create a hardened base image for various cloud virtualization platforms using Ansible, Ansible Automation Platform and Packer.

Packer is dispositioned to use OS isos to build servers with either kickstart configuration files for Linux-based servers and auto unattended files for Windows-based servers. Kickstart and Auto Unattended files can do a lot things during the initialization like add and remove packages, create users, partition disk, etc. It does have its limitations. After those files are done, they cannot be used again without rebuilding the entire images.

Using Ansible can help here to finish out the build and allow the configuration files to remain in their simpliest state. Ansible can harden the image and coded to with roles that can use on the image and after it been deployed to keep the hardening in place. It can be variablized to allow change that need to happen over time to made easily. Having reusability is one of strengths.

This section will show how Packer will build the initial image, then it will reach out to Ansible Automation Platform to finish up the initial image to install and remove packages, setup and disable users, permissions and fortifications so the image at rest is already hardened before it's deployed in the cloud environment.

The other part of this story is the Packer and Automation workflow is able to produce hardened base images for all types of cloud platforms like VMware, Azure, Amazon and Oracle Cloud.

== Process Flow

// [mermaid]
// ....
// graph TD
//   A((Start)) --> B(((Ansible Contoller)))
//   style A fill:green,stroke:#333,stroke-width:6px
//   B --> |Image Build Part One| C[Run Packer Base OS Image Job Template]
//   C --> |Ansible Survey Questions| D(calls Role-Packer-OS-Base-Image)
//   D --> E[/OS Type drives what comes next/]
//   E --> |Linux| F(Packer starts building with Linux HCL Configuration)
//   E --> |Windows| G(Packer starts building with Windows HCL Configuration)
//   F --> |Linux image is built from ISO| H[Packer Ansible Provisioner runs configure_from_aap_controller.yml]
//   H --> |Image Build Part Two| B
//   B --> |Post Image Build| I[/OS Type determines which Workflow Job Template is executed/]
//   I --> |Linux Post Build| J[Run Base-Image-Linux-Setup-WFJT]
//   J --> |Run Linux Post Build Job Templates| K[Run Role-RedHat-Register]
//   K --> L[Apply CIS Rules to Image via CIS Collection to Linux Image]
//   L --> M[Run other Roles to apply cloud destination packages, etc.]
//   M --> |Image Done| Q
//   I --> |Windows Post Build| O[/Run Windows Part One or Part Two/]
//   O --> |Update Only| P[Run Windows-2019-Base-Image-Part-One-WFJT]
//   P --> |Image Done| Q[Convert to Template]
//   O --> |Complate Windows Build| R[Run Windows-2019-Base-Image-Part-Two-WFJT]
//   R --> S[Apply CIS Rules to Image via CIS Collection to Windows Image]
//   S --> T[Run other Roles to apply cloud destination packages, etc.]
//   T --> Q
//   Q --> |All Done| U((Done))
//   style U fill:red,stroke:#333,stroke-width:6px
// ....

== Requirements

The following is a list of requirements that makes this process work:

* A custom execution environment will be required that has Packer installed. The details are in the `Ansible Custom Execution Environment` section below.

== Components

== Ansible Custom Execution Environment

The following are files that were used to create the custom execution environment:

The `ansible.cfg` will allow the ansible builder to have access to the Private Automation to pull down collections, etc.

.custom-execution-environment/ansible.cfg
[source,yaml,source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/ansible.cfg[]
----

This file determines which collection will be preloaded on to the custom execution environment.

.custom-execution-environment/requirements.yml
[source,yaml,source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/requirements.yml[]
----

This file will make sure required python modules for roles and collections of roles are installed.

.custom-execution-environment/requirements.txt
[source,ini,source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/requirements.txt[]
----

The system entry in the definition points to a bindep requirements file, which will install system-level dependencies that are outside of what the collections already include as their dependencies. It may be listed as a relative path from the directory of the automation execution environment definition’s folder, or an absolute path.

.custom-execution-environment/bindep.txt
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/bindep.txt[]
----

The `execution-environment.yml` is the main file that tells the Ansible Builder how to build the custom execution environment:

.custom-execution-environment/execution-environment.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/custom-execution-environment/execution-environment.yml[]
----

After these file were created and it's recommended that these file are storaged in a repository under version control. These files will be updated over time to update the version of all the components that on this execution environment. The next step is to build the new custom execution environment. The following are the step used to build the custom execution environment:

Steps:

. Login into the Private Automation Hub's registry using `podman`:
+
.Log into Private Automation Hub registry.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
$ podman login -u=username automation-hub-url
----

. Pull down a current Execution Environment like ee-supported-rhel8:latest:
+
.Use podman to pull ee-supported-rhel8:latest.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
podman pull https://{automation_hub}/ee-supported-rhel8:latest
----

. Login into the Private Automation Hub's registry using `podman`:
+
.Build the new execution environment with ansible-builder.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
$ ansible-builder build --file execution-environment.yml --verbosity 3 --container-runtime=podman --tag custom-supported-ee:1.0 --tag custom-supported-ee:latest
----

. Obtain the IMAGE ID for the image that the ansible-builder just generated:
+
.Obtain the image id for the new custom execution environment.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
$ podman push images
----

. Pick the image id for `localhost/custom-supported-ee`. Push the new execution environment to the Hub using the image id:
+
.Log into Private Automation Hub registry.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
$ podman push <image_id> automationhub/custom-supported-ee
----

. Log into the Controller to add the custom execution environment.

. Click Execution Environments.

. Click **Add** to add new environment.

. Fill in **Name** with `Custom Supported EE`.

. Fill in **Image** with `{automation_hub}/custom-suppor-ee:latest`.

. Select `Always pull container before running` for **Pull**.

. Fill in **Description** with `Use for packer image builds`.

. Leave **Organaization** blank.

. Select `Automation Hub Container Registry` for **Registry credential**.

The Custom Execution Environment is ready to use for building images.

== Inventory Variable Setup

The best way to allow this process to be changed over time is to use variables in all the roles, packer templates, ansible templates, etc. This requires the setup of inventory that will use in the Controller that has variables. The inventory is a project repository that is called into inventory as a source.

The directory tree for inventory look like the following:

[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
.
├── group_vars
│   ├── all
│   │   └── 00-shared.yml
│   ├── linux
│   │   ├── 00-registration.yml
│   │   ├── 00-rhel7-cis.yml
│   │   ├── 00-rhel8-cis.yml
│   │   └── 00-shared-linux.yml
│   ├── win2019
│   │   └── 00-windows-2019-cis.yml
│   └── windows
│       ├── 00-shared-windows.yml
│       └── all.yml
├── host_vars
└── inventory
----

The inventory is setup so there are two groups that used to provision images. Those groups are `linux` and `windows`. Looking at the inventory directory list above, windows has two groups because windows server by the nature have different building requirements. `window2019` is one example that has variable specific to that image. The general `windows` variables file hold variables that are common amoungst all windows images.

Linux could have been done the same way to split them out for images like RHEL, OEL, Ubuntu, etc. For most cases, Linux variations have very little if any uncommon settings other then repository differences.

The other files in those groups of OSes are for different packages that need to be preloaded on the image to make it available for deployment later in Cloud infrastructure.

If the file is only under `linux` then it's only used during linux builds and that's same for windows.

Let look at some of key files in the inventory variable source.

=== File: `00-shared.yml`

This file contains variables for Jinja2 templates that will create Packer Template HCL files, Kickstart Configuration Files, etc.

.00-shared.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-shared.yml[]
----

=== File: `00-shared-linux.yml``

.00-shared-linux.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-shared-linux.yml[]
----

=== File: `00-shared-windows.yml``

.00-shared-windows.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-shared-windows.yml[]
----

=== File: `inventory`

This file is used by Inventory Sources, that has been set to `Sourced from a Project`. The inventory variables are upload to the controller as a Git repository via a Project.  Once the project is synced then a Inventory Source can be create, using `inventory` as `Inventory File` setting.

.inventory.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/inventory[]
----

=== File: `00-rhel7-cis.yml`

This inventory file is used by the role `RHEL7-CIS` which is from `https://github.com/ansible-lockdown/RHEL7-CIS`. There are variable in this file that need to configured so Ansible and Packer is able to build the RHEL 7 image. Some of the rule were disabled to keep the Image from stopping Ansible from completing it's build. When the image is actually use, the role will need to executed again, with those rules enabled as the last role after all the connection to LDAP, application are installed, etc.

.00-rhel7-cis.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-rhel7-cis.yml[]
----

=== File: `00-rhel8-cis.yml`

his inventory file is used by the role `RHEL8-CIS` which is from `https://github.com/ansible-lockdown/RHEL8-CIS`. There are variable in this file that need to configured so Ansible and Packer is able to build the RHEL 8 image. Some of the rule were disabled to keep the Image from stopping Ansible from completing it's build. When the image is actually use, the role will need to executed again, with those rules enabled as the last role after all the connection to LDAP, application are installed, etc.

.00-rhel8-cis.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-rhel8-cis.yml[]
----

=== File: `00-windows-2019-cis.yml``

.00-windows-2019-cis.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/inventory_files/00-windows-2019-cis.yml[]
----

== Ansible Project and Roles

The process of building the image consists of two parts a project and it calls a role using the variables above and the custom execution environment that has packer pre-installed. The project or playbook has all the Job Templates that will be called by Ansible Controller. The role is doing all image building, the project is there call the image build role and how the post-image setup playbooks that are used by the Job Templates.

Each Job Template in the project call in various Workflow Job Templates in a pre-determined order to finish the image setup. After the image is setup then it's converted back to a VMware template. The VMware template can be converted later to be used on other clouds, but initially it's all build on VMware.

The `site.yml`` in the project called `Packer-OS-Base-Image` contains the following:

.'Packer-OS-Base-Image/site.yml`.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/projects/Packer-OS-Base-Image/site.yml[]
----

Once the Job Template is executed, it will use the variables from the Job Template Survey question and call the role `Role-Packer-OS-Base-Image`. This role uses Packer to build the image, then called Ansible Controller to call Workflow Job Templates to setup the image be hardened, etc.

.'Role-Packer-OS-Base-Image/main.yml`.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/tasks/main.yml[]
----

When looking through the code for the role, there are two sections that setup HCL, ks.cfg, etc. files on the execution environment for Linux and one section for Windows.  The templates for those configuration are taken from roles template directory. The rendered configuration files will be used by Packer to build out the Base image. There are Packer HCL templates for several OSes, but for this document, showing how the Linux ones work and finally the windows to learn the concept and flow.

One of the primary configuration file that needs to be rendered is a Packer HCL build file that end in `pkr.hcl`. The Linux HCL file looks like the following:

.'Role-Packer-OS-Base-Image/templates/rhel7.pkr.hcl.j2`.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/rhel7.pkr.hcl.j2[]
----

The `rhel7.pkr.hcl.j2` is rendered on the custom execution environment as `rhel7.pkr.hcl`.  It will tell Packer where to place the base images, the configuration of the base images like memory, cpu, disk, etc.  It will call the OS ISO file already on the vSphere server and create a virtual ISO with the kickstart configuration file inside it.  The kickstart configuration looks like the follow:

.'Role-Packer-OS-Base-Image/templates/rhel7.cfg.j2`.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/rhel7.cfg.j2[]
----

Both of the Jinja 2 based template rely heavily on the variables that are list in inventory variable file `00-shared.yml` that was listed in the inventory section early in this document.

The last section of the Packer HCL configuration file does a `packer init`, `packer validation` and finally a `packer build` to start the base image OS build on the VMware cloud. Inside the Packer HCL configuration file is a section in the build that call the Ansible Controller to have the Controller run Workflow Job Templates against the base OS image to do hardening, and other configuration as needed.  This is the section of the `rhel7.pkr.hcl` file does the API call to Ansible Controller:

.'Role-Packer-OS-Base-Image/templates/rhel7.pkr.hcl.j2`.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/rhel7.pkr.hcl.j2[lines=112..130]
----

The API call is done with another rendered template called `configure_from_aap_controller.yml.j2`. This file will do the actual call of the Workflow Job Templates that gets to post base image build configured.

.'Role-Packer-OS-Base-Image/templates/configure_from_aap_controller.yml`.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/configure_from_aap_controller.yml[]
----

The file is broken up into sections for different cloud environments. This one just shows the VMware cloud configuration.

The Windows is the same that was described above, but one key file that has changed and takes the place of the kickstart configuration file. Windows uses a `autounttended.xml` file to configure Windows:

.'Role-Packer-OS-Base-Image/templates/autounattended.xml.2019.j2`
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/autounattend.xml.2019.j2[]
----

There are several example of this file on the internet or one can be created by using the following references:

* https://learn.microsoft.com/en-us/windows-hardware/manufacture/desktop/update-windows-settings-and-scripts-create-your-own-answer-file-sxs?view=windows-10[Update Windows Settins and Scripts Create Your Own Answer File SXS]
* https://learn.microsoft.com/en-us/windows-hardware/customize/desktop/unattend/[Unattended Windows Setup Reference]
* https://www.windowsafg.com/[Windows Answer File Generator]
* https://www.windowsafg.com/server2019.html[Windows Answer File Generator for Server 2019]

The other item that answer file does is runs various scripts to configure the Windows base image further before Ansible takes over. This script are place on the custom execution environment as well and are included CDROM that packer builds. For this image build the follow script were called from the ansible file:

.'Role-Packer-OS-Base-Image/templates/scripts/disable-network-discovery.cmd.j2`
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/scripts/disable-network-discovery.cmd.j2[]
----

.'Role-Packer-OS-Base-Image/templates/scripts/disable-winrm.ps1.j2`
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/scripts/disable-winrm.ps1.j2[]
----

.'Role-Packer-OS-Base-Image/templates/scripts/enable-rdp.cmd.j2`
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/scripts/enable-rdp.cmd.j2[]
----

.'Role-Packer-OS-Base-Image/templates/scripts/enable-winrm.ps1.j2`
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/scripts/enable-winrm.ps1.j2[]
----

.'Role-Packer-OS-Base-Image/templates/scripts/install-vm-tools.cmd.j2`
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/scripts/install-vm-tools.cmd.j2[]
----

.'Role-Packer-OS-Base-Image/templates/scripts/set-temp.ps1.j2`
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/scripts/set-temp.ps1.j2[]
----

.'Role-Packer-OS-Base-Image/templates/scripts/winrm-ansible.ps1.j2`
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Packer-OS-Base-Image/templates/scripts/winrm-ansible.ps1.j2[]
----

There are two steps for Windows, the first round will create a Windows OS base image that is fully patches. The second round will call the same steps as the Linux process by calling Workflow Job Template in Ansible Controller.

== Other Playbooks in Packer-OS-Base-Image Project

Hardening the Base OS Image is one of the principles of this procedure. There are few playbooks in directory that will harden the image, listed here a few keys ones:

* Register and Un-Register Linux for Updates
* Patch Linux
* Windows Update
* CIS
* Linux Partition Mount Hardening
* Various Post Configuration that installs agents, setups, etc. different cloud environments.

Some of the key playbooks are listed next:

.'Packer-OS-Base-Image/register.yml'
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/projects/Packer-OS-Base-Image/register.yml[]
----

.'Packer-OS-Base-Image/unregister.yml'
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/projects/Packer-OS-Base-Image/unregister.yml[]
----

.'Packer-OS-Base-Image/patch.yml'
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/projects/Packer-OS-Base-Image/patch.yml[]
----

.'Packer-OS-Base-Image/cis.yml'
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/projects/Packer-OS-Base-Image/cis.yml[]
----

.'Packer-OS-Base-Image/windows_update.yml'
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/projects/Packer-OS-Base-Image/windows_update.yml[]
----

== Ansible Controller

The other items that Ansible Controller besides Job Templates, Workflow Job Templates for Base OS Image build are:

* Inventory with all the Host Records for each Base OS Image that will be built.
* Inventory source that pointing to the inventory variable project in git.
* A number of credentials have been setup to allow the controller to run packer, access the image and other resources.

== Variables

The inventory variable files contain a lot of variables to list here and the files list in prior section have outlined many of the variables that have been used.

== How to make changes and updates

For the most part, the only changes that will happen will be following:

* Adding a new OS type which will require new sections in existing dictionaries found the inventory files and most `00-shared.yml`.
* Update existing OS ISO names when new version become available.
* Add new roles for post build that will require a playbook in the project `Packer-OS-Base-Image`, new Job Template pointing to those playbooks and inclusion of those Job Template into Workflow Job Templates.

When any inventory variable is update, sign on the Ansible Controller and manually update the inventory project that will cause the inventory source to update.

== Final Thoughts

== Roles and Code Snippets used in the Image Builds

=== Packer Base OS Image Job Template

This Job Template can be import into Ansible Controller using `awx import` which is part of the `awxkit` python module.

.Base Image OS Templates Configuration
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/job_templates/Base-Image-OS-Job-Template.yml[]
----

=== Role Snippets

==== Role-Hardened-Mounts

.Role-Hardened-Mounts/tasks/main.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Hardened-Mounts/tasks/main.yml[]
----

==== Role-Patch

.Role-Patch/tasks/main.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Patch/tasks/main.yml[]
----

.Role-Patch/defaults/main.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Patch/defaults/main.yml[]
----

.Role-Patch/handlers/main.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Patch/handlers/main.yml[]
----

==== Role-Redhat-Register

.Role-Redhat-Register/tasks/main.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Redhat-Register/tasks/main.yml[]
----

.Role-Redhat-Register/defaults/main.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Redhat-Register/defaults/main.yml[]
----

==== Role-Redhat-Unregister

.Role-Redhat-Unregister/tasks/main.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Redhat-register/tasks/main.yml[]
----

==== Role-Windows-Update

.Role-Windows-Update/tasks/main.yml
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/roles/Role-Windows-Update/tasks/main.yml[]
----

=== Ansible Lockdown Collection

For the CIS playbook and its Job Template in Ansible Controller a collection was created to pull down repository from https://github.com/ansible-lockdown[Ansible Lockdown]. The following repositories were add to the collection as submodules to make them easier to update when Ansible Lockdown updates the roles:

* https://github.com/ansible-lockdown/RHEL7-CIS
* https://github.com/ansible-lockdown/RHEL8-CIS
* https://github.com/ansible-lockdown/Windows-2019-CIS

To create submodules, refer to https://git-scm.com/book/en/v2/Git-Tools-Submodules[7.11 Git Tools - Submodules]

Since Ansible doesn't support hyphens in role names, the following command was used to add the repositories to the roles directory with a name change:

.Add submodules with a name change.
[source,bash,linenums,options="nowrap",subs="attributes,verbatim"]
----
git submodule add --name windows_2019_cis git@github.com:ansible-lockdown/Windows-2019-CIS.git roles/window_2019_cis
git submodule add --name rhel7_cis git@github.com:ansible-lockdown/RHEL7-CIS.git roles/rhel7_cis
git submodule add --name rhel8_cis git@github.com:ansible-lockdown/RHEL8-CIS.git roles/rhel8_cis
----

The `.gitmodules` will look like the following:

.submodules.
[source,ini,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/collections/CIS/.gitmodules[]
----

Create the `galaxy.yml` like the following:

.galaxy.yml.
[source,yaml,linenums,options="nowrap",subs="attributes,verbatim"]
----
include::code/collections/CIS/galaxy.yml[]
----

Lastly, the collection needs to added to the Private Automation Hub or present in collections directory of the Packer-OS-Base-Image Playbook.

== Final Thoughts

The design was structured so as much as possible to allow changes could be done with changing variables in the inventory. If a new OS is added or new post image build configurations are needed then code changes will have to happen. Using this section, should help in allow those changes to go easier.
